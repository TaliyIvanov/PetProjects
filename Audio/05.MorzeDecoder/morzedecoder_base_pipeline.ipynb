{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80297639",
   "metadata": {},
   "source": [
    "### О данном ноутбуке\n",
    "\n",
    "В данном ноутбуке я займусь тем, что реализую свою нейроночку, написав ее самостоятельно с поддержкой Grok))), для обучения декодировать код Морзе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f186c14",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bfe2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import librosa \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from collections import defaultdict\n",
    "import uuid\n",
    "import glob\n",
    "from scipy import signal\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4abb97",
   "metadata": {},
   "source": [
    "### Преднастройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26a41230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальная длительность: 48.00 секунд\n",
      "Медианная длительность: 8.00 секунд\n",
      "Средняя длительность: 8.02 секунд\n",
      "Количество обработанных файлов: 35000\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 8000  # Частота дискретизации\n",
    "DATA_DIR = 'data/morse_dataset/morse_dataset'  # Путь к папке с файлами *.opus\n",
    "\n",
    "def audio_duration_stats():\n",
    "    \"\"\"Находит максимальную, медианную и среднюю длительность аудиофайлов в папке.\"\"\"\n",
    "    durations = []\n",
    "    \n",
    "    # Получаем список всех *.opus файлов в директории\n",
    "    opus_files = glob.glob(os.path.join(DATA_DIR, '*.opus'))\n",
    "    \n",
    "    if not opus_files:\n",
    "        print(f\"В папке {DATA_DIR} не найдено *.opus файлов\")\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    for file_path in opus_files:\n",
    "        try:\n",
    "            # Загружаем аудиофайл без ограничения длительности\n",
    "            audio, _ = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "            # Вычисляем длительность в секундах\n",
    "            duration = len(audio) / SAMPLE_RATE\n",
    "            durations.append(duration)\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при обработке файла {file_path}: {e}\")\n",
    "    \n",
    "    if durations:\n",
    "        max_duration = max(durations)\n",
    "        median_duration = np.median(durations)\n",
    "        mean_duration = np.mean(durations)\n",
    "        \n",
    "        print(f\"Максимальная длительность: {max_duration:.2f} секунд\")\n",
    "        print(f\"Медианная длительность: {median_duration:.2f} секунд\")\n",
    "        print(f\"Средняя длительность: {mean_duration:.2f} секунд\")\n",
    "        print(f\"Количество обработанных файлов: {len(durations)}\")\n",
    "        \n",
    "        return max_duration, median_duration, mean_duration\n",
    "    else:\n",
    "        print(\"Не удалось обработать ни один файл\")\n",
    "        return 0, 0, 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    max_duration, median_duration, mean_duration = audio_duration_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c33082b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "SAMPLE_RATE = 8000 # Downsample to reduce memory usage\n",
    "DURATION = 15\n",
    "N_MELS = 64\n",
    "MAX_SEQ_LEN = 50\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "DATA_DIR = 'data/morse_dataset/morse_dataset'\n",
    "# DEBUG_TRAIN_SIZE = 100  # Number of training samples for debug\n",
    "# DEBUG_TEST_SIZE = 20   # Number of test samples for debug\n",
    "\n",
    "# Russian alphabet + digits + special characters\n",
    "CHAR_MAP = {\n",
    "    'А': '.-', 'Б': '-...', 'В': '.--', 'Г': '--.', 'Д': '-..', 'Е': '.', 'Ё': '.', \n",
    "    'Ж': '...-', 'З': '--..', 'И': '..', 'Й': '.---', 'К': '-.-', 'Л': '.-..', \n",
    "    'М': '--', 'Н': '-.', 'О': '---', 'П': '.--.', 'Р': '.-.', 'С': '...', \n",
    "    'Т': '-', 'У': '..-', 'Ф': '..-.', 'Х': '....', 'Ц': '-.-.', 'Ч': '---.', \n",
    "    'Ш': '----', 'Щ': '--.-', 'Ъ': '-..-', 'Ы': '-.--', 'Ь': '-..-', 'Э': '..-..', \n",
    "    'Ю': '..--', 'Я': '.-.-', '0': '-----', '1': '.----', '2': '..---', \n",
    "    '3': '...--', '4': '....-', '5': '.....', '6': '-....', '7': '--...', \n",
    "    '8': '---..', '9': '----.', ' ': ' ', '#': '#'\n",
    "}\n",
    "CHAR_TO_INT = {char: idx + 1 for idx, char in enumerate(CHAR_MAP.keys())}  # 1-based indexing\n",
    "CHAR_TO_INT[''] = 0  # Blank for CTC\n",
    "INT_TO_CHAR = {v: k for k, v in CHAR_TO_INT.items()}\n",
    "NUM_CLASSES = len(CHAR_TO_INT)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c777f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кастомный класс для создания датасетам\n",
    "class MorseDataset(Dataset):\n",
    "    \"\"\"Dataset for Morse code audio and transcripts.\"\"\"\n",
    "    def __init__(self, df, data_dir, is_train=True):\n",
    "        self.df = df\n",
    "        self.data_dir = data_dir\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.df.iloc[idx]['id'])\n",
    "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=20)  # Remove silence\n",
    "        mel = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=N_MELS)\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "        mel_db = (mel_db - np.mean(mel_db)) / np.std(mel_db)\n",
    "        spectrogram = torch.FloatTensor(mel_db).unsqueeze(0)  # Add channel dimension\n",
    "        if self.is_train:\n",
    "            transcript = self.df.iloc[idx]['message']\n",
    "            label = []\n",
    "            for c in transcript:\n",
    "                if c not in CHAR_TO_INT:\n",
    "                    print(f\"Warning: Symbol '{c}' in transcript '{transcript}' (id: {self.df.iloc[idx]['id']}) not in CHAR_MAP\")\n",
    "                else:\n",
    "                    label.append(CHAR_TO_INT[c])\n",
    "            label = torch.LongTensor(label)\n",
    "            return spectrogram, label, len(label), mel_db.shape[1]\n",
    "        return spectrogram, self.df.iloc[idx]['id'], mel_db.shape[1]\n",
    "\n",
    "# функция с полосовым фильтром для очистки спектрограмм\n",
    "# мне кажется, что шумы убираться не стоит\n",
    "    # def __getitem__(self, idx):\n",
    "    #     file_path = os.path.join(self.data_dir, self.df.iloc[idx]['id'])\n",
    "    #     audio, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "    #     audio, _ = librosa.effects.trim(audio, top_db=30)\n",
    "        \n",
    "    #     sos = signal.butter(5, [500, 1000], btype='band', fs=SAMPLE_RATE, output='sos')\n",
    "    #     audio = signal.sosfilt(sos, audio)\n",
    "        \n",
    "    #     max_samples = int(DURATION * SAMPLE_RATE)\n",
    "    #     if len(audio) > max_samples:\n",
    "    #         audio = audio[:max_samples]\n",
    "        \n",
    "    #     mel = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=N_MELS)\n",
    "    #     mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    #     mel_db = (mel_db - np.mean(mel_db)) / np.std(mel_db)\n",
    "    #     spectrogram = torch.FloatTensor(mel_db).unsqueeze(0)\n",
    "    #     if self.is_train:\n",
    "    #         transcript = self.df.iloc[idx]['message']\n",
    "    #         label = [CHAR_TO_INT[c] for c in transcript if c in CHAR_TO_INT]\n",
    "    #         label = torch.LongTensor(label)\n",
    "    #         return spectrogram, label, len(label), mel_db.shape[1]\n",
    "    #     return spectrogram, self.df.iloc[idx]['id'], mel_db.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd721149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    if len(batch[0]) == 4:  # Train\n",
    "        spectrograms, labels, label_lengths, spec_lengths = zip(*batch)\n",
    "        max_width = max(spec_lengths)\n",
    "        padded_specs = [F.pad(spec, (0, max_width - spec.size(2))) for spec in spectrograms]\n",
    "        spectrograms = torch.stack(padded_specs)\n",
    "        labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "        label_lengths = torch.LongTensor(label_lengths)\n",
    "        spec_lengths = torch.LongTensor(spec_lengths)\n",
    "        return spectrograms, labels, label_lengths, spec_lengths\n",
    "    else:  # Test\n",
    "        spectrograms, file_ids, spec_lengths = zip(*batch)\n",
    "        max_width = max(spec_lengths)\n",
    "        padded_specs = [F.pad(spec, (0, max_width - spec.size(2))) for spec in spectrograms]\n",
    "        spectrograms = torch.stack(padded_specs)\n",
    "        spec_lengths = torch.LongTensor(spec_lengths)\n",
    "        # print(f\"Test batch: len(spectrograms)={len(spectrograms)}, len(file_ids)={len(file_ids)}\")\n",
    "        return spectrograms, file_ids, spec_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "227408f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель\n",
    "class MorseModel(nn.Module):\n",
    "    \"\"\"CNN-LSTM model for Morse code decoding.\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super(MorseModel, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(32 * (N_MELS // 4), 64, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(128, num_classes)  # 128 = 64 * 2 (bidirectional)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        batch, channels, height, width = x.size()\n",
    "        x = x.permute(0, 3, 1, 2).contiguous().view(batch, width, channels * height)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8cdf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # такая архитектура слишком сложная\n",
    "# class MorseModel(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(MorseModel, self).__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),\n",
    "#             nn.Dropout(0.2)\n",
    "#         )\n",
    "#         self.lstm = nn.LSTM(64 * (N_MELS // 8), 128, bidirectional=True, batch_first=True)\n",
    "#         self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         batch, channels, height, width = x.size()\n",
    "#         x = x.permute(0, 3, 1, 2).contiguous().view(batch, width, channels * height)\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7647b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(preds):\n",
    "    decoded = []\n",
    "    # print(f\"decode_predictions: preds shape={preds.shape}\")\n",
    "    preds = preds.permute(1, 0, 2)  # [T, N, C] -> [N, T, C]\n",
    "    for pred in preds:\n",
    "        pred = pred.argmax(dim=-1).cpu().numpy()\n",
    "        text = []\n",
    "        last = None\n",
    "        for p in pred:\n",
    "            if p != last and p != 0:\n",
    "                text.append(INT_TO_CHAR[p])\n",
    "            last = p\n",
    "        decoded.append(''.join(text))\n",
    "    # print(f\"decode_predictions: len(decoded)={len(decoded)}\")\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1711ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_mean(y_true, y_pred):\n",
    "    \"\"\"Compute mean Levenshtein distance.\"\"\"\n",
    "    distances = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        dist = levenshtein_distance(true, pred)\n",
    "        distances.append(dist / max(len(true), 1))\n",
    "    return np.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "faa41059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for spectrograms, labels, label_lengths, spec_lengths in dataloader:\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        spec_lengths = spec_lengths.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(spectrograms).log_softmax(2)\n",
    "            input_lengths = spec_lengths // 4  # Account for two MaxPool2d(2) layers\n",
    "            # отладка\n",
    "            # for batch_labels in labels:\n",
    "            #     for i in batch_labels:\n",
    "            #         if i.item() not in INT_TO_CHAR and i != 0:\n",
    "            #             print(f\"Invalid label index: {i.item()}\")\n",
    "\n",
    "            loss = criterion(outputs.permute(1, 0, 2), labels, input_lengths, label_lengths)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cfe2e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, labels, label_lengths, spec_lengths in dataloader:\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(spectrograms).log_softmax(2)\n",
    "            pred_texts = decode_predictions(outputs.permute(1, 0, 2))\n",
    "            true_texts = [''.join(INT_TO_CHAR[i.item()] for i in label if i != 0 and i.item() in INT_TO_CHAR) for label in labels]\n",
    "            preds.extend(pred_texts)\n",
    "            trues.extend(true_texts)\n",
    "    return levenshtein_mean(trues, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00613c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    train_df = pd.read_csv('data/train.csv')\n",
    "    test_df = pd.read_csv('data/test.csv')\n",
    "    \n",
    "    # Limit to debug size\n",
    "    # train_df = train_df.head(DEBUG_TRAIN_SIZE)\n",
    "    # test_df = test_df.head(DEBUG_TEST_SIZE)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MorseDataset(train_df, DATA_DIR, is_train=True)\n",
    "    test_dataset = MorseDataset(test_df, DATA_DIR, is_train=False)\n",
    "    \n",
    "    # Split train into train and validation\n",
    "    val_size = int(0.1 * len(train_dataset))  # 10 samples\n",
    "    train_size = len(train_dataset) - val_size  # 90 samples\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = MorseModel(NUM_CLASSES).to(device)\n",
    "    criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Training loop with earlystopping\n",
    "    #start_time = time.time() # Засекаем время начала тренировки модели\n",
    "    best_val_lev = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, scaler)\n",
    "        val_lev = evaluate(model, val_loader)\n",
    "        scheduler.step(val_lev)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Levenshtein Mean: {val_lev:.4f}\")\n",
    "        \n",
    "        # Засекаем время конца обучения эпохи\n",
    "        #end_time = time.time()\n",
    "        #epoch_time = end_time - start_time\n",
    "        #print(f\"Время на обучение модели в эпоху {epoch + 1}: {epoch_time:.2f} секунд.\")\n",
    "\n",
    "        if val_lev < best_val_lev:\n",
    "            best_val_lev = val_lev\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    \n",
    "    # Predict on test set\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_file_paths = []\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, file_ids, spec_lengths in test_loader:\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(spectrograms).log_softmax(2)\n",
    "            pred_texts = decode_predictions(outputs.permute(1, 0, 2))\n",
    "            # print(f\"Batch: len(file_ids)={len(file_ids)}, len(pred_texts)={len(pred_texts)}\")\n",
    "            test_preds.extend(pred_texts)\n",
    "            test_file_paths.extend(file_ids)\n",
    "\n",
    "    # Debug lengths\n",
    "    # print(f\"Total: len(test_file_paths)={len(test_file_paths)}, len(test_preds)={len(test_preds)}\")\n",
    "\n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_file_paths,\n",
    "        'message': test_preds\n",
    "    })\n",
    "    # Save submission\n",
    "    submission.to_csv('submission.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b3a8b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 4.1314, Val Levenshtein Mean: 1.0000\n",
      "Epoch 2/50, Train Loss: 4.0066, Val Levenshtein Mean: 1.0000\n",
      "Epoch 3/50, Train Loss: 3.9421, Val Levenshtein Mean: 0.9615\n",
      "Epoch 4/50, Train Loss: 3.8477, Val Levenshtein Mean: 0.9585\n",
      "Epoch 5/50, Train Loss: 3.2295, Val Levenshtein Mean: 0.7167\n",
      "Epoch 6/50, Train Loss: 2.3924, Val Levenshtein Mean: 0.5707\n",
      "Epoch 7/50, Train Loss: 2.0012, Val Levenshtein Mean: 0.4926\n",
      "Epoch 8/50, Train Loss: 1.7569, Val Levenshtein Mean: 0.4395\n",
      "Epoch 9/50, Train Loss: 1.5795, Val Levenshtein Mean: 0.4012\n",
      "Epoch 10/50, Train Loss: 1.4387, Val Levenshtein Mean: 0.3741\n",
      "Epoch 11/50, Train Loss: 1.3310, Val Levenshtein Mean: 0.3514\n",
      "Epoch 12/50, Train Loss: 1.2483, Val Levenshtein Mean: 0.3376\n",
      "Epoch 13/50, Train Loss: 1.1769, Val Levenshtein Mean: 0.3243\n",
      "Epoch 14/50, Train Loss: 1.1188, Val Levenshtein Mean: 0.3179\n",
      "Epoch 15/50, Train Loss: 1.0664, Val Levenshtein Mean: 0.2996\n",
      "Epoch 16/50, Train Loss: 1.0241, Val Levenshtein Mean: 0.2943\n",
      "Epoch 17/50, Train Loss: 0.9874, Val Levenshtein Mean: 0.2864\n",
      "Epoch 18/50, Train Loss: 0.9509, Val Levenshtein Mean: 0.2855\n",
      "Epoch 19/50, Train Loss: 0.9205, Val Levenshtein Mean: 0.2761\n",
      "Epoch 20/50, Train Loss: 0.8925, Val Levenshtein Mean: 0.2767\n",
      "Epoch 21/50, Train Loss: 0.8667, Val Levenshtein Mean: 0.2702\n",
      "Epoch 22/50, Train Loss: 0.8433, Val Levenshtein Mean: 0.2706\n",
      "Epoch 23/50, Train Loss: 0.8190, Val Levenshtein Mean: 0.2676\n",
      "Epoch 24/50, Train Loss: 0.7980, Val Levenshtein Mean: 0.2647\n",
      "Epoch 25/50, Train Loss: 0.7765, Val Levenshtein Mean: 0.2617\n",
      "Epoch 26/50, Train Loss: 0.7602, Val Levenshtein Mean: 0.2590\n",
      "Epoch 27/50, Train Loss: 0.7394, Val Levenshtein Mean: 0.2591\n",
      "Epoch 28/50, Train Loss: 0.7229, Val Levenshtein Mean: 0.2549\n",
      "Epoch 29/50, Train Loss: 0.7056, Val Levenshtein Mean: 0.2573\n",
      "Epoch 30/50, Train Loss: 0.6883, Val Levenshtein Mean: 0.2535\n",
      "Epoch 31/50, Train Loss: 0.6723, Val Levenshtein Mean: 0.2583\n",
      "Epoch 32/50, Train Loss: 0.6571, Val Levenshtein Mean: 0.2569\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 37\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m patience_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 37\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     val_lev \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n\u001b[1;32m     39\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(val_lev)\n",
      "Cell \u001b[0;32mIn[62], line 4\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, scaler)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m spectrograms, labels, label_lengths, spec_lengths \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      5\u001b[0m     spectrograms, labels \u001b[38;5;241m=\u001b[39m spectrograms\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m     spec_lengths \u001b[38;5;241m=\u001b[39m spec_lengths\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/media/talium/1DA5AE943A305AF1/DataSciense/Projects/PetProjects/Audio/05.MorzeDecoder/morzeenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/media/talium/1DA5AE943A305AF1/DataSciense/Projects/PetProjects/Audio/05.MorzeDecoder/morzeenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/media/talium/1DA5AE943A305AF1/DataSciense/Projects/PetProjects/Audio/05.MorzeDecoder/morzeenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/media/talium/1DA5AE943A305AF1/DataSciense/Projects/PetProjects/Audio/05.MorzeDecoder/morzeenv/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/media/talium/1DA5AE943A305AF1/DataSciense/Projects/PetProjects/Audio/05.MorzeDecoder/morzeenv/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[54], line 14\u001b[0m, in \u001b[0;36mMorseDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     13\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m     audio, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLE_RATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     audio, _ \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39meffects\u001b[38;5;241m.\u001b[39mtrim(audio, top_db\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)  \u001b[38;5;66;03m# Remove silence\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     mel \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmelspectrogram(y\u001b[38;5;241m=\u001b[39maudio, sr\u001b[38;5;241m=\u001b[39mSAMPLE_RATE, n_mels\u001b[38;5;241m=\u001b[39mN_MELS)\n",
      "File \u001b[0;32m/media/talium/1DA5AE943A305AF1/DataSciense/Projects/PetProjects/Audio/05.MorzeDecoder/morzeenv/lib/python3.10/site-packages/librosa/core/audio.py:175\u001b[0m, in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Otherwise try soundfile first, and then fall back if necessary\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m         y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n",
      "File \u001b[0;32m/media/talium/1DA5AE943A305AF1/DataSciense/Projects/PetProjects/Audio/05.MorzeDecoder/morzeenv/lib/python3.10/site-packages/librosa/core/audio.py:221\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[0;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[1;32m    218\u001b[0m         frame_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# Load the target number of frames, and transpose to match librosa form\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43msf_desc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_duration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, sr_native\n",
      "File \u001b[0;32m/media/talium/1DA5AE943A305AF1/DataSciense/Projects/PetProjects/Audio/05.MorzeDecoder/morzeenv/lib/python3.10/site-packages/soundfile.py:895\u001b[0m, in \u001b[0;36mSoundFile.read\u001b[0;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frames \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m frames \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(out):\n\u001b[1;32m    894\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[0;32m--> 895\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_array_io\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mread\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m>\u001b[39m frames:\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/media/talium/1DA5AE943A305AF1/DataSciense/Projects/PetProjects/Audio/05.MorzeDecoder/morzeenv/lib/python3.10/site-packages/soundfile.py:1344\u001b[0m, in \u001b[0;36mSoundFile._array_io\u001b[0;34m(self, action, array, frames)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39msizeof(ctype)\n\u001b[1;32m   1343\u001b[0m cdata \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mcast(ctype \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m, array\u001b[38;5;241m.\u001b[39m__array_interface__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 1344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cdata_io\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/talium/1DA5AE943A305AF1/DataSciense/Projects/PetProjects/Audio/05.MorzeDecoder/morzeenv/lib/python3.10/site-packages/soundfile.py:1353\u001b[0m, in \u001b[0;36mSoundFile._cdata_io\u001b[0;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[1;32m   1351\u001b[0m     curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1352\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_snd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m action \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m ctype)\n\u001b[0;32m-> 1353\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1354\u001b[0m _error_check(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_errorcode)\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9342e50",
   "metadata": {},
   "source": [
    "Epoch 1/50, Train Loss: 4.1314, Val Levenshtein Mean: 1.0000\n",
    "Epoch 2/50, Train Loss: 4.0066, Val Levenshtein Mean: 1.0000\n",
    "Epoch 3/50, Train Loss: 3.9421, Val Levenshtein Mean: 0.9615\n",
    "Epoch 4/50, Train Loss: 3.8477, Val Levenshtein Mean: 0.9585\n",
    "Epoch 5/50, Train Loss: 3.2295, Val Levenshtein Mean: 0.7167\n",
    "Epoch 6/50, Train Loss: 2.3924, Val Levenshtein Mean: 0.5707\n",
    "Epoch 7/50, Train Loss: 2.0012, Val Levenshtein Mean: 0.4926\n",
    "Epoch 8/50, Train Loss: 1.7569, Val Levenshtein Mean: 0.4395\n",
    "Epoch 9/50, Train Loss: 1.5795, Val Levenshtein Mean: 0.4012\n",
    "Epoch 10/50, Train Loss: 1.4387, Val Levenshtein Mean: 0.3741\n",
    "Epoch 11/50, Train Loss: 1.3310, Val Levenshtein Mean: 0.3514\n",
    "Epoch 12/50, Train Loss: 1.2483, Val Levenshtein Mean: 0.3376\n",
    "Epoch 13/50, Train Loss: 1.1769, Val Levenshtein Mean: 0.3243\n",
    "Epoch 14/50, Train Loss: 1.1188, Val Levenshtein Mean: 0.3179\n",
    "Epoch 15/50, Train Loss: 1.0664, Val Levenshtein Mean: 0.2996\n",
    "Epoch 16/50, Train Loss: 1.0241, Val Levenshtein Mean: 0.2943\n",
    "Epoch 17/50, Train Loss: 0.9874, Val Levenshtein Mean: 0.2864\n",
    "Epoch 18/50, Train Loss: 0.9509, Val Levenshtein Mean: 0.2855\n",
    "Epoch 19/50, Train Loss: 0.9205, Val Levenshtein Mean: 0.2761\n",
    "Epoch 20/50, Train Loss: 0.8925, Val Levenshtein Mean: 0.2767\n",
    "Epoch 21/50, Train Loss: 0.8667, Val Levenshtein Mean: 0.2702\n",
    "Epoch 22/50, Train Loss: 0.8433, Val Levenshtein Mean: 0.2706\n",
    "Epoch 23/50, Train Loss: 0.8190, Val Levenshtein Mean: 0.2676\n",
    "Epoch 24/50, Train Loss: 0.7980, Val Levenshtein Mean: 0.2647\n",
    "Epoch 25/50, Train Loss: 0.7765, Val Levenshtein Mean: 0.2617\n",
    "...\n",
    "Epoch 29/50, Train Loss: 0.7056, Val Levenshtein Mean: 0.2573\n",
    "Epoch 30/50, Train Loss: 0.6883, Val Levenshtein Mean: 0.2535\n",
    "Epoch 31/50, Train Loss: 0.6723, Val Levenshtein Mean: 0.2583\n",
    "Epoch 32/50, Train Loss: 0.6571, Val Levenshtein Mean: 0.2569\n",
    "\n",
    "Модель начала переобучаться на 31ой эпохе, поэтому было принято решение остановить обучение, сделать сабмишн и посмотреть какой максимальный результат я смогу тут выжать)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60ff41de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Функция для предсказаний на тестовой выборке\n",
    "def predict_test():\n",
    "    # Загрузка тестовых данных\n",
    "    test_df = pd.read_csv('data/test.csv')\n",
    "    \n",
    "    # Создание датасета и загрузчика\n",
    "    test_dataset = MorseDataset(test_df, DATA_DIR, is_train=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    # Инициализация модели\n",
    "    model = MorseModel(NUM_CLASSES).to(device)\n",
    "    \n",
    "    # Загрузка лучшей модели\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    # Предсказания\n",
    "    test_preds = []\n",
    "    test_file_paths = []\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, file_ids, spec_lengths in test_loader:\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            with autocast():\n",
    "                outputs = model(spectrograms).log_softmax(2)\n",
    "            pred_texts = decode_predictions(outputs.permute(1, 0, 2))\n",
    "            test_preds.extend(pred_texts)\n",
    "            test_file_paths.extend(file_ids)\n",
    "    \n",
    "    # Создание submission\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_file_paths,\n",
    "        'message': test_preds\n",
    "    })\n",
    "    submission.to_csv('submission.csv', encoding='utf-8', index=False)\n",
    "    print(\"Submission file created: submission.csv\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    predict_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa2339",
   "metadata": {},
   "source": [
    "Очень удивительно, конечно)\n",
    "\n",
    "Я обучал модельку всю ночь, ее показатель на метрике упал с 0,8 до 0,25, а публичный Score с 6,87 до 2,53. С 72ого места я поднялся аж на целое 67ое!)\n",
    "\n",
    "По сути или я что то делаю не так (что весьме вероятно) или надо менять подход)))\n",
    "\n",
    "Видимо, мне не хватает опыта, знаний и понимания процесса, ну чтож, тем не менее это был хороший опыт с экспериментиками)\n",
    "\n",
    "На этом данный ноутбук подойдет к своему финалу, а я попробую перейти на пайплайн с предобученной моделью WAV2VEC =)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "morzeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
